<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />



<title></title>

<script src="report_files/jquery-1.11.0/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link href="report_files/bootstrap-2.3.2/css/journal.min.css" rel="stylesheet" />
<link href="report_files/bootstrap-2.3.2/css/bootstrap-responsive.min.css" rel="stylesheet" />
<script src="report_files/bootstrap-2.3.2/js/bootstrap.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="report_files/highlight/default.css"
      type="text/css" />
<script src="report_files/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
</style>
<div class="container-fluid main-container">




<div id="predictive-modeling-on-human-activity-recognition" class="section level1">
<h1>Predictive Modeling on Human Activity Recognition</h1>
<p><em>Li Yutze,</em> 19 Mar 2015</p>
<hr />
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Wearable devices can monitor our daily movement on multi-dimensional ways, employing delicate accelerometers, gyrometers and other electronic sensors. They have been producing massive ammount of data to be processed, encoding underlying information about the users of these devices. A traditional way of interperating the data is to use the data to predict the type of movement the user is performing. Here we used <code>R</code> (3.1.3) and the <code>caret</code> package and built an accurate predictive model to address this task, as a assignment of Coursera course <a href="https://class.coursera.org/predmachlearn-012"><em>Practical Machine Learning</em></a>.</p>
<p>The data is download directly from the course webpage, which is a preprocessed version of the raw data reported in the project of <em>Velloso, E.</em> et al. In Velloso’s program, a group of tester was required to perform 5 type of movements (here simplified as capital letter A ~ E), with sensor device positioned on 4 part of the body and a set of time sequence data is produced.</p>
</div>
<div id="data-structure-and-data-manupulation" class="section level2">
<h2>Data Structure and Data Manupulation</h2>
<p>First set up wirking environmnet and attach required packages.</p>
<pre class="r"><code>&gt; setwd(&quot;~/CodeRClass/Learning&quot;)
&gt; require(caret)</code></pre>
<pre><code>## Loading required package: caret
## Loading required package: lattice
## Loading required package: ggplot2</code></pre>
<pre class="r"><code>&gt; require(lattice)
&gt; require(reshape)</code></pre>
<pre><code>## Loading required package: reshape</code></pre>
<p>Now load the data sets into our working environment. Notice that for some columns in the <code>csv</code> file, missing values are represented by empty strings (i.e. <code>''</code>), so we add <code>na.strings = ''</code> to specify these columns.</p>
<pre class="r"><code>&gt; data_train &lt;- read.csv(&#39;./pml-training.csv&#39;,na.strings = &#39;&#39;)
&gt; data_test &lt;- read.csv(&#39;./pml-testing.csv&#39;, na.strings = &#39;&#39;)
&gt; names(data_train)[1:20]</code></pre>
<pre><code>##  [1] &quot;X&quot;                    &quot;user_name&quot;            &quot;raw_timestamp_part_1&quot;
##  [4] &quot;raw_timestamp_part_2&quot; &quot;cvtd_timestamp&quot;       &quot;new_window&quot;          
##  [7] &quot;num_window&quot;           &quot;roll_belt&quot;            &quot;pitch_belt&quot;          
## [10] &quot;yaw_belt&quot;             &quot;total_accel_belt&quot;     &quot;kurtosis_roll_belt&quot;  
## [13] &quot;kurtosis_picth_belt&quot;  &quot;kurtosis_yaw_belt&quot;    &quot;skewness_roll_belt&quot;  
## [16] &quot;skewness_roll_belt.1&quot; &quot;skewness_yaw_belt&quot;    &quot;max_roll_belt&quot;       
## [19] &quot;max_picth_belt&quot;       &quot;max_yaw_belt&quot;</code></pre>
<p>Within all variables in the data, we found some of them are ‘window summaries’ that only have valid value in rows that <code>$new_window == 'yes'</code>. These variables are kurtoses, skewnesses, maximums and minimums, amplitude, and statistical values like variances, averages, and standard deviations. Since the prediction of human motion should acctually based on ‘instance variables’ that are measured contemporaneously, so we need to remove these variables before doing analysis.</p>
<pre class="r"><code>&gt; class_info &lt;- data_train$classe
&gt; vars &lt;- names(data_train)
&gt; # Remove general information variables and the class variable
&gt; misc_vars &lt;- vars %in% c(&#39;X&#39;, &#39;user_name&#39;, &#39;raw_timestamp_part_1&#39;,
+                          &#39;raw_timestamp_part_2&#39;, &#39;cvtd_timestamp&#39;,
+                          &#39;new_window&#39;, &#39;num_window&#39;, &#39;classe&#39;)
&gt; # Remove window-summaries, using grep()
&gt; window_vars &lt;- grepl(&#39;kurtosis|skewness|max|min|amplitude|var|avg|stddev&#39;, vars)
&gt; 
&gt; data_train &lt;- data_train[!(misc_vars | window_vars)]
&gt; data_test &lt;- data_test[!(misc_vars | window_vars)]
&gt; # Test any NA in the data
&gt; any(sapply(data_train, function(x) any(is.na(x))))</code></pre>
<pre><code>## [1] FALSE</code></pre>
<p>Now that our data tables are <code>NA</code> free and notice that we copied the class column out and leaved only predictors in our data frames.</p>
</div>
<div id="partitioning-preprocessing-and-simple-eda" class="section level2">
<h2>Partitioning, Preprocessing and Simple EDA</h2>
<div id="data-partitioning-and-cross-validation-method" class="section level3">
<h3>Data Partitioning and Cross Validation Method</h3>
<p>To estimate the out-of-sample ER, we need to further cut the ‘training’ data into a second level training and testing data. Here we used the <code>createDatePartition()</code> and extract 60% of the original training data as the cross-validating training data, and the rest as testing subset.</p>
<pre class="r"><code>&gt; set.seed(1)
&gt; train_ind &lt;- createDataPartition(class_info, p = 0.6, list = F)
&gt; 
&gt; training &lt;- data_train[train_ind, ]
&gt; training_class &lt;- class_info[train_ind]
&gt; 
&gt; testing &lt;- data_train[-train_ind, ]
&gt; testing_class &lt;- class_info[-train_ind]</code></pre>
<p>For further statistical learning process, we will use the <code>training</code> set to train the model, and use the <code>testing</code> set to check the out-of-sample error rate.</p>
</div>
<div id="exploratory-data-analysis-and-data-preprocessing" class="section level3">
<h3>Exploratory Data Analysis and Data Preprocessing</h3>
<pre class="r"><code>&gt; vars &lt;- names(training)
&gt; vars[1:21]</code></pre>
<pre><code>##  [1] &quot;roll_belt&quot;        &quot;pitch_belt&quot;       &quot;yaw_belt&quot;        
##  [4] &quot;total_accel_belt&quot; &quot;gyros_belt_x&quot;     &quot;gyros_belt_y&quot;    
##  [7] &quot;gyros_belt_z&quot;     &quot;accel_belt_x&quot;     &quot;accel_belt_y&quot;    
## [10] &quot;accel_belt_z&quot;     &quot;magnet_belt_x&quot;    &quot;magnet_belt_y&quot;   
## [13] &quot;magnet_belt_z&quot;    &quot;roll_arm&quot;         &quot;pitch_arm&quot;       
## [16] &quot;yaw_arm&quot;          &quot;total_accel_arm&quot;  &quot;gyros_arm_x&quot;     
## [19] &quot;gyros_arm_y&quot;      &quot;gyros_arm_z&quot;      &quot;accel_arm_x&quot;</code></pre>
<p>Currently in our data, we have 2 groups of variables, one group is the instant measurements on 3 dimensions on 4 body location (belt, arm, forearm and dumbbel) of 3 physical quantities (accelerate, angular accelerate and displacement magnitude), and the other group is the summary of these 3-d values (indicated by <code>roll_</code>, <code>pitch_</code>, <code>yaw_</code> and <code>total_accel_</code>)</p>
<p>For sure the correlations accross the two groups of variables are considerable, so here we compare the representativeness on the type movement accross the groups using simple EDA plots.</p>
<pre class="r"><code>&gt; stripout &lt;- function(keyword, ...) {
+     for (i in 1:length(keyword)) {
+         vars_sub &lt;- grepl(keyword[i], vars)
+         sub &lt;- training[, vars_sub]
+         long &lt;- data.frame(value = unlist(sub),
+                            variable = rep(names(sub), each = nrow(sub)),
+                            class = rep(training_class, times = ncol(sub)))
+         print(stripplot(class ~ value | variable, groups = class,
+                           data = long, alpha = 0.01, pch = 19,
+                           auto.key = F, ...))
+     }
+ }</code></pre>
<p>These function takes a regular expression string as query input, and generate the stripplots accross variables follows that pattern. We first check the summay group variables (<code>$roll_</code>, <code>$pitch_</code>, <code>$yaw_</code> and <code>$total_accel_</code>).</p>
<pre class="r"><code>&gt; stripout(&#39;belt$&#39;, layout = c(4, 1), relation = &#39;free&#39;)</code></pre>
<div class="figure">
<img src="report_files/figure-html/unnamed-chunk-7-1.png" />
</div>
<p>Here we only shows the summary group variables on the <code>belt</code> for simplification, you can check for other query inputs like <code>'^accel'</code> and <code>'^roll'</code>, etc.</p>
<p>We then checked the instant 3-d variables:</p>
<pre class="r"><code>&gt; stripout(&#39;(^acc).*(x$)&#39;, layout = c(4, 1), relation = &#39;free&#39;)</code></pre>
<div class="figure">
<img src="report_files/figure-html/unnamed-chunk-8-1.png" />
</div>
<p>Since both group can explain the motion type on the same extend, and the summary group has higher information density (say, for the 3-d group, the <code>_x</code> and <code>_y</code> can be merged into one for walking which is the displacment on the x-y plane)</p>
<pre class="r"><code>&gt; vars_summaries &lt;- grepl(&#39;roll|pitch|yaw|total&#39;, vars)
&gt; sub_summaries &lt;- training[, vars_summaries]</code></pre>
<p>Then we checked that if we used the compacked summary group would we still need to perform principle component analysis (singular value decomposition) before training. Here we manually did a SVD. The percent variance and cumulative percent variance explained by each components are plotted as following:</p>
<pre class="r"><code>&gt; s &lt;- svd(t(as.matrix(sub_summaries)))
&gt; d &lt;- s$d
&gt; dss &lt;- d ** 2 / sum(d ** 2) * 100
&gt; cumdss &lt;- cumsum(dss)
&gt; long &lt;- data.frame(
+     Variance = c(dss, cumdss),
+     Component = rep(1:length(dss), 2),
+     type = factor(rep(c(&#39;absolute&#39;, &#39;cumulative&#39;), each = length(dss)),
+                   levels = c(&#39;cumulative&#39;, &#39;absolute&#39;))
+ )
&gt; xyplot(Variance ~ Component, groups = type, data = long, 
+        type = c(&#39;s&#39;, &#39;h&#39;), ylim = c(0, 100), auto.key = T)</code></pre>
<div class="figure">
<img src="report_files/figure-html/unnamed-chunk-10-1.png" />
</div>
<p>From the plot of percent variance explained, it was not hard to find the PCA was not efficient, that the first 4 components sum up explained less than 80% of the total variance. And form the following plots that compared the clusterring of movement types on x-y plots of PCA components and original variables, we can also find the PCA did not work well for spread away different classed data points</p>
<pre class="r"><code>&gt; recon &lt;- d * t(s$v)
&gt; xyplot(recon[1, ] + recon[2, ] ~ recon[3, ], groups = training_class,
+        alpha = 0.05, pch = 19, xlab = NULL, ylab = NULL,
+        strip = strip.custom(
+            factor.levels = c(&#39;PC1 - PC2&#39;, &#39;PC2 - PC3&#39;)))</code></pre>
<div class="figure">
<img src="report_files/figure-html/unnamed-chunk-11-1.png" />
</div>
<pre class="r"><code>&gt; xyplot(sub_summaries[, 1] + sub_summaries[, 2] ~ sub_summaries[, 5],
+        groups = training_class, alpha = 0.05, pch = 19, xlab = NULL, ylab = NULL,
+        strip = strip.custom(
+            factor.levels = c(&#39;roll_belt - pitch_belt&#39;, &#39;roll_belt - roll_armt&#39;)))</code></pre>
<div class="figure">
<img src="report_files/figure-html/unnamed-chunk-11-2.png" />
</div>
</div>
</div>
<div id="modeling-and-prediction" class="section level2">
<h2>Modeling and Prediction</h2>
<p>Based on the reasoning in the EDA chapter, here we dicided to train the model with the summary group variable in the training data set without PCA as input. Because the projected 2-D patterns shows the distribution of variables within each class do not follows multi-dimensional normal distribution, here we employed the boosting regression method (<code>gbm</code>) to build the model.</p>
<pre class="r"><code>&gt; training_s &lt;- training[vars_summaries]
&gt; testing_s &lt;- testing[vars_summaries]
&gt; 
&gt; set.seed(1)
&gt; fit &lt;- train(training_class ~ ., training_s, method = &#39;gbm&#39;, verbose = F)</code></pre>
<p>After built the model, we checked the out-of-sample ER using <code>confusisonMatrix()</code>.</p>
<pre class="r"><code>&gt; pred_train &lt;- predict(fit, training_s)
&gt; confusionMatrix(training_class, pred_train)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 3270   47   12   13    6
##          B   64 2087  105   12   11
##          C    6   91 1918   38    1
##          D   10   11   43 1862    4
##          E    6   29   23   33 2074
## 
## Overall Statistics
##                                          
##                Accuracy : 0.952          
##                  95% CI : (0.948, 0.9558)
##     No Information Rate : 0.285          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      
##                                          
##                   Kappa : 0.9393         
##  Mcnemar&#39;s Test P-Value : 1.149e-08      
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9744   0.9214   0.9129   0.9510   0.9895
## Specificity            0.9907   0.9798   0.9859   0.9931   0.9906
## Pos Pred Value         0.9767   0.9158   0.9338   0.9648   0.9580
## Neg Pred Value         0.9898   0.9813   0.9812   0.9902   0.9977
## Prevalence             0.2850   0.1923   0.1784   0.1663   0.1780
## Detection Rate         0.2777   0.1772   0.1629   0.1581   0.1761
## Detection Prevalence   0.2843   0.1935   0.1744   0.1639   0.1838
## Balanced Accuracy      0.9826   0.9506   0.9494   0.9720   0.9901</code></pre>
<pre class="r"><code>&gt; pred_test &lt;- predict(fit, testing_s)
&gt; confusionMatrix(testing_class, pred_test)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 2147   49   20    8    8
##          B   61 1331  107   12    7
##          C    5   77 1248   36    2
##          D    3    6   42 1228    7
##          E    4   26   15   24 1373
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9339          
##                  95% CI : (0.9281, 0.9393)
##     No Information Rate : 0.2829          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9164          
##  Mcnemar&#39;s Test P-Value : 1.429e-07       
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9671   0.8939   0.8715   0.9388   0.9828
## Specificity            0.9849   0.9706   0.9813   0.9911   0.9893
## Pos Pred Value         0.9619   0.8768   0.9123   0.9549   0.9521
## Neg Pred Value         0.9870   0.9750   0.9716   0.9878   0.9963
## Prevalence             0.2829   0.1898   0.1825   0.1667   0.1781
## Detection Rate         0.2736   0.1696   0.1591   0.1565   0.1750
## Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838
## Balanced Accuracy      0.9760   0.9322   0.9264   0.9650   0.9861</code></pre>
<p>The <code>gbm</code> method tend to be a little overfitting, but here the accuracy on the testing set is just near it on the training set, indicating that the out-of-sample ER (= 0.018) is not high.</p>
<pre class="r"><code>&gt; test_mat &lt;- as.matrix(confusionMatrix(testing_class, pred_test))
&gt; test_mat &lt;- round(apply(test_mat, 2, function(x) x / sum(x)), 3)
&gt; levelplot(test_mat, col.regions = gray(seq(1, 0, -0.01)),
+           xlab = &#39;Prediction&#39;, ylab = &#39;Reference&#39;,
+           panel = function(...) {
+               panel.levelplot(...)
+               panel.text(x = rep(1:5, each = 5), y = rep(1:5, 5),
+                          labels = as.character(test_mat),
+                          col = ifelse(test_mat &gt; .6, &#39;white&#39;, &#39;black&#39;))
+           })</code></pre>
<div class="figure">
<img src="report_files/figure-html/unnamed-chunk-14-1.png" />
</div>
<p>And this plot shows the accuracies in the testing set, the darkness of the cell <code>(i, j)</code> is the precent of cases which a sample is called <code>i</code> and acctually is of class <code>j</code>. The dark diagnal in the plot indicates the high accuracy of our model.</p>
<p>Using our model to predict the 20-sample test data, the result was:</p>
<pre class="r"><code>&gt; query &lt;- data_test[vars_summaries]
&gt; answer &lt;- predict(fit, query)
&gt; answer</code></pre>
<pre><code>##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E</code></pre>
<hr />
<div id="reference" class="section level4">
<h4>Reference</h4>
<ul>
<li>Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human ’13) . Stuttgart, Germany: ACM SIGCHI, 2013. Visit <a href="http://groupware.les.inf.puc-rio.br/har">the websit</a></li>
</ul>
</div>
<div id="programming-environment" class="section level4">
<h4>Programming Environment</h4>
<ul>
<li>System: OS X Yusemite 10.10.2</li>
<li>R Version: 3.1.3</li>
</ul>
</div>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
